{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/onwood/SBA_project_3D\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "from utils.mydataset import MyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'dataset'\n",
    "dataset = MyDataset(data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69624"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['A073', 'A086', 'A046', 'A078', 'A066', 'A033', 'A024', 'A038', 'A025', 'A031', 'A084', 'A105', 'A072', 'A011', 'A047', 'A063', 'A094', 'A019', 'A071', 'A003', 'A041', 'A010', 'A067', 'A030', 'A032', 'A085', 'A044', 'A082', 'A064', 'A077', 'A036', 'A088', 'A020', 'A016', 'A062', 'A065', 'A017', 'A013', 'A029', 'A061', 'A021', 'A006', 'A034', 'A079', 'A068', 'A012', 'A028', 'A083', 'A091', 'A004', 'A090', 'A070', 'A074', 'A087', 'A043', 'A069', 'A014', 'A093', 'A015', 'A103', 'A075', 'A049', 'A081', 'A001', 'A005', 'A018', 'A042', 'A037', 'A045', 'A076', 'A089', 'A002', 'A092'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.action_dir_dic.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('dataset/a073_kp/S028C002P087R001A073_rgb_kp.json',\n",
       " 'dataset/a073_kp/S030C002P075R001A073_rgb_kp.json',\n",
       " 'dataset/a084_kp/S028C001P070R002A084_rgb_kp.json',\n",
       " 'dataset/a103_kp/S018C002P008R001A103_rgb_kp.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.all_data_dir_list[0], dataset.all_data_dir_list[90], dataset.all_data_dir_list[10201], dataset.all_data_dir_list[56704]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 12])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_item = dataset[10201]\n",
    "data_item.keys()\n",
    "data_item['semi_positives_c0'][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([425, 1080, 1080])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_item['heatmap'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anchor_c0 torch.Size([1, 25, 12])\n",
      "anchor_aug_c0 torch.Size([1, 27, 12])\n",
      "semi_positives_c0 torch.Size([10, 41, 12])\n",
      "anchor_c1 torch.Size([1, 25, 4])\n",
      "anchor_aug_c1 torch.Size([1, 27, 4])\n",
      "semi_positives_c1 torch.Size([10, 41, 4])\n",
      "anchor_c2 torch.Size([1, 25, 4])\n",
      "anchor_aug_c2 torch.Size([1, 27, 4])\n",
      "semi_positives_c2 torch.Size([10, 41, 4])\n",
      "anchor_c3 torch.Size([1, 25, 4])\n",
      "anchor_aug_c3 torch.Size([1, 27, 4])\n",
      "semi_positives_c3 torch.Size([10, 41, 4])\n",
      "anchor_c4 torch.Size([1, 25, 4])\n",
      "anchor_aug_c4 torch.Size([1, 27, 4])\n",
      "semi_positives_c4 torch.Size([10, 41, 4])\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"anchor_c{i}\", data_item[f\"anchor_c{i}\"].shape)\n",
    "    print(f\"anchor_aug_c{i}\", data_item[f\"anchor_aug_c{i}\"].shape)\n",
    "    print(f\"semi_positives_c{i}\", data_item[f\"semi_positives_c{i}\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_keypoint(c0, c1, c2, c3, c4, a):\n",
    "    __, K, D = c0.shape\n",
    "    os.mkdir('test/semi/'+a.split('/')[-1].split('.')[0])\n",
    "    \n",
    "    neck_keypoint = np.array([5.0, 5.0]).reshape(1, 1, 2)\n",
    "    neck_keypoint = np.tile(neck_keypoint, [1, K, 1])\n",
    "    \n",
    "    nose = c0[:, :, :2] + neck_keypoint\n",
    "    left_shoulder = c0[:, :, 2:4] + neck_keypoint\n",
    "    right_shoulder = c0[:, :, 4:6] + neck_keypoint\n",
    "    mid_hip = c0[:, :, 6:8] + neck_keypoint\n",
    "    left_hip = c0[:, :, 8:10] + mid_hip\n",
    "    right_hip = c0[:, :, 10:12] + mid_hip\n",
    "\n",
    "    left_elbow = c1[:, :, 0:2]  + left_shoulder\n",
    "    left_wrist = c1[:, :, 2:4] + left_elbow\n",
    "\n",
    "    right_elbow = c2[:, :, 0:2]  + right_shoulder\n",
    "    right_wrist = c2[:, :, 2:4] + right_elbow\n",
    "\n",
    "    left_knee = c3[:, :, 0:2] + left_hip\n",
    "    left_ankle = c3[:, :, 2:4] + left_knee\n",
    "\n",
    "    right_knee = c4[:, :, 0:2] + right_hip\n",
    "    right_ankle = c4[:, :, 2:4] + right_knee\n",
    "\n",
    "    restored_keypoints = np.stack([\n",
    "        nose,\n",
    "        neck_keypoint,\n",
    "        left_shoulder,\n",
    "        right_ankle,\n",
    "        mid_hip,\n",
    "        left_hip,\n",
    "        right_hip,\n",
    "        left_elbow,\n",
    "        left_wrist,\n",
    "        right_elbow,\n",
    "        right_wrist,\n",
    "        left_knee,\n",
    "        left_ankle,\n",
    "        right_knee,\n",
    "        right_ankle\n",
    "    ], axis=-2)\n",
    "\n",
    "    print(restored_keypoints.shape)\n",
    "    print(restored_keypoints.max(), restored_keypoints.min())\n",
    "\n",
    "    for frame_index in range(restored_keypoints.shape[1]):\n",
    "        canvas = np.zeros((10, 10))\n",
    "\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "        ax.imshow(canvas)\n",
    "\n",
    "        for keypoint_index in range(restored_keypoints.shape[2]):\n",
    "            ax.add_patch(patches.Circle(\n",
    "                [restored_keypoints[0, frame_index, keypoint_index][1], restored_keypoints[0, frame_index, keypoint_index][0]],\n",
    "                radius=0.1,\n",
    "                fill=True,\n",
    "                color=\"red\"\n",
    "            ))\n",
    "        \n",
    "        fn = 'test/semi/'+a.split('/')[-1].split('.')[0]+'/'+str(frame_index)\n",
    "        plt.savefig(fn)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_anchor(index):\n",
    "    data_item = dataset[index]\n",
    "    visualize_keypoint(\n",
    "        *[data_item[f\"anchor_c{i}\"] for i in range(5)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_anchor_aug(index):\n",
    "    a, data_item = dataset[index]\n",
    "    visualize_keypoint(\n",
    "        *[data_item[f\"anchor_aug_c{i}\"] for i in range(5)], a\n",
    "    )\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 11:44:36.319846: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-17 11:44:37.320175: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_semi(index):\n",
    "    a, data_item = dataset[index]\n",
    "    for j in range(len(a)):\n",
    "        visualize_keypoint(\n",
    "            *[data_item[f\"semi_positives_c{i}\"][j][tf.newaxis,:,:] for i in range(5)], a[j]\n",
    "        )\n",
    "        print(a[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset/a073_kp/S026C002P077R002A073_rgb_kp.json', 'dataset/a073_kp/S023C003P066R002A073_rgb_kp.json', 'dataset/a073_kp/S025C001P055R001A073_rgb_kp.json', 'dataset/a073_kp/S027C003P006R002A073_rgb_kp.json', 'dataset/a073_kp/S030C002P041R001A073_rgb_kp.json', 'dataset/a073_kp/S026C001P044R002A073_rgb_kp.json', 'dataset/a073_kp/S024C002P064R001A073_rgb_kp.json', 'dataset/a073_kp/S029C001P075R001A073_rgb_kp.json', 'dataset/a073_kp/S020C002P053R002A073_rgb_kp.json', 'dataset/a073_kp/S030C003P044R002A073_rgb_kp.json']\n"
     ]
    }
   ],
   "source": [
    "a, data_item = dataset[0]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('dataset/a073_kp/S028C002P087R001A073_rgb_kp.json',\n",
       " 'dataset/a073_kp/S030C002P075R001A073_rgb_kp.json',\n",
       " 'dataset/a046_kp/S007C003P007R001A046_rgb_kp.json',\n",
       " 'dataset/a084_kp/S028C001P070R002A084_rgb_kp.json',\n",
       " 'dataset/a103_kp/S018C002P008R001A103_rgb_kp.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.all_data_dir_list[0], dataset.all_data_dir_list[90], dataset.all_data_dir_list[2345], dataset.all_data_dir_list[10201], dataset.all_data_dir_list[56704]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_semi(3444)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints_from_a_json(origin_vid_dir):\n",
    "    with open(origin_vid_dir, \"r\") as f:\n",
    "        origin_keypoints = json.load(f)\n",
    "\n",
    "    origin_keypoints_anno = origin_keypoints['annotations']\n",
    "    origin_tmp_list = []\n",
    "    for frame_data in origin_keypoints_anno:\n",
    "        if frame_data:\n",
    "            keypoints_list = [frame_data[key] for key in frame_data if frame_data[key]]\n",
    "            if keypoints_list:\n",
    "                origin_tmp_list.extend(keypoints_list)\n",
    "\n",
    "    # # Nonetype error 발생하는 듯\n",
    "    # if not origin_tmp_list:\n",
    "    #     return None\n",
    "\n",
    "    # reshape x\n",
    "    origin_anchor_keypoints = np.array(origin_tmp_list)[:,:,[0,1]]\n",
    "    return origin_anchor_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(points, h, w , sigma):\n",
    "    x = points[:,[1]][0]\n",
    "    y = points[:,[0]][0]\n",
    "    channel = [np.exp(-((w - x) ** 2 + (h - y) ** 2) / (2 * sigma ** 2))]\n",
    "    # channel = np.reshape(channel, newshape=(h,w))\n",
    "    return channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.])]\n"
     ]
    }
   ],
   "source": [
    "data = extract_keypoints_from_a_json('dataset/a001_kp/S001C001P001R001A001_rgb_kp.json')\n",
    "points = data.reshape(-1,2)\n",
    "\n",
    "h = 1080\n",
    "w = 1920\n",
    "\n",
    "sigma = 5\n",
    "\n",
    "channel = gaussian(points, h, w, sigma)\n",
    "\n",
    "print(channel)\n",
    "\n",
    "# x_points = x.reshape(-1,)\n",
    "# y_points = y.reshape(-1,)\n",
    "\n",
    "# x_gau = gaussian(x_points, mean, sigma)\n",
    "# mu_x = 0.5 * (x + 1.) * w\n",
    "# mu_y = 0.5 * (y + 1.) * h\n",
    "\n",
    "# tmp_size = sigma * 3\n",
    "\n",
    "# x1, y1 = mu_x - tmp_size, mu_y - tmp_size\n",
    "\n",
    "# print(x_gau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N, H, W = 12, 10, 10\n",
    "# landmarks = torch.randint(0, H, (N, 2))\n",
    "\n",
    "# tmp = torch.zeros(N, 3, H, W)\n",
    "# rows, cols = landmarks.split(1, 1)\n",
    "# tmp[torch.arange(N), :, rows, cols] = 1.\n",
    "\n",
    "# image = torch.randn(N, 3, H, W)\n",
    "# image = torch.cat((image, tmp), 1)\n",
    "# print(image.shape)\n",
    "# > torch.Size([12, 6, 10, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   1   2 ... 253 254 255]\n",
      " [  0   1   2 ... 253 254 255]\n",
      " [  0   1   2 ... 253 254 255]\n",
      " ...\n",
      " [  0   1   2 ... 253 254 255]\n",
      " [  0   1   2 ... 253 254 255]\n",
      " [  0   1   2 ... 253 254 255]]\n"
     ]
    }
   ],
   "source": [
    "kp = (100, 150) # W,H\n",
    "kp2 = (100, 180)\n",
    "H = np.tile(np.arange(256), 256).reshape(256,256).T\n",
    "W = np.tile(np.arange(256), 256).reshape(256,256)\n",
    "\n",
    "std = 5\n",
    "x_gauss = 1/(np.sqrt(2*np.pi)*std) * np.exp(-0.5*((H-kp[1])/std)**2) \n",
    "y_gauss = 1/(np.sqrt(2*np.pi)*std) * np.exp(-0.5*((W-kp[0])/std)**2)\n",
    "frame1 = 1000*(x_gauss * y_gauss)\n",
    "\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints_from_a_json(origin_vid_dir):\n",
    "    with open(origin_vid_dir, \"r\") as f:\n",
    "        origin_keypoints = json.load(f)\n",
    "\n",
    "    origin_keypoints_anno = origin_keypoints['annotations']\n",
    "    origin_tmp_list = []\n",
    "    for frame_data in origin_keypoints_anno:\n",
    "        if frame_data:\n",
    "            keypoints_list = [frame_data[key] for key in frame_data if frame_data[key]]\n",
    "            if keypoints_list:\n",
    "                origin_tmp_list.extend(keypoints_list)\n",
    "\n",
    "    # # Nonetype error 발생하는 듯\n",
    "    # if not origin_tmp_list:\n",
    "    #     return None\n",
    "\n",
    "    # reshape x\n",
    "    origin_anchor_keypoints = np.array(origin_tmp_list)[:,:,[0,1]]\n",
    "    return origin_anchor_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "data = extract_keypoints_from_a_json('dataset/a001_kp/S001C001P001R001A001_rgb_kp.json')\n",
    "points = data.reshape(-1,2)\n",
    "x_points = points[:,[0]]\n",
    "y_points = points[:,[1]]\n",
    "\n",
    "h = 1080\n",
    "w = 1920*(1080/1920)\n",
    "\n",
    "H = np.tile(np.arange(h), h).reshape(h,h).T\n",
    "W = np.tile(np.arange(w), h).reshape(h,h)\n",
    "\n",
    "std = 5\n",
    "x = x_points[0]\n",
    "y = y_points[0]*(1080/1920)\n",
    "x_gauss = 1/(np.sqrt(2*np.pi)*std) * np.exp(-0.5*((H-x)/std)**2) \n",
    "y_gauss = 1/(np.sqrt(2*np.pi)*std) * np.exp(-0.5*((W-y)/std)**2)\n",
    "frame1 = 1000*(x_gauss * y_gauss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "data = extract_keypoints_from_a_json('dataset/a001_kp/S001C001P001R001A001_rgb_kp.json')\n",
    "points = data.reshape(-1,2)\n",
    "x_points = points[:,[0]]\n",
    "y_points = points[:,[1]]\n",
    "\n",
    "h = 1080\n",
    "w = 1920*(1080/1920)\n",
    "\n",
    "H = np.tile(np.arange(h), h).reshape(h,h).T\n",
    "W = np.tile(np.arange(w), h).reshape(h,h)\n",
    "std = 5\n",
    "\n",
    "org_image = Image.open('frame0.jpg')\n",
    "resized_image = org_image.resize((int(w), h))\n",
    "\n",
    "for i in range(17):\n",
    "    x = x_points[i]\n",
    "    y = y_points[i]*(1080/1920)\n",
    "    x_gauss = 1/(np.sqrt(2*np.pi)*std) * np.exp(-0.5*((H-x)/std)**2) \n",
    "    y_gauss = 1/(np.sqrt(2*np.pi)*std) * np.exp(-0.5*((W-y)/std)**2)\n",
    "    frame = (x_gauss * y_gauss)\n",
    "\n",
    "    plt.imshow(resized_image)\n",
    "    plt.imshow(frame, alpha = 0.6, cmap = 'viridis')\n",
    "\n",
    "    plt.savefig(f'overlap/frame{i}.png', bbox_inches='tight', pad_inches=0)\n",
    "    # overlap = frame * 0.4 + resized_image\n",
    "\n",
    "    # overlap_pil = Image.fromarray(overlap)\n",
    "    # overlap_pil.show()\n",
    "\n",
    "    # overlap_pil.save(f'overlap/frame{i}.png','png')\n",
    "\n",
    "    # ax = sns.heatmap(overlap)\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
